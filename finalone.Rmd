---
title: "practical homework - 1"
output: pdf_document
---

---

```{r}

#Imported all the necessary libraries
library(tidyverse)
library(ISLR2)
library(tree)
library(randomForest)
library(gbm)
library(caret)

# I've set the working directory and loaded 'youth_data.Rdata' into data
setwd("C:/Users/alekh/Downloads/")
data <- load("youth_data.Rdata")
data
df
youth_experience_cols
substance_cols
demographic_cols
# Named the dataframe 'df' as drug_use
drug_use <- na.omit(df)
drug_use

# PART1: BINARY CLASSIFICATION: Predicting whether a youth has ever consumed alcohol or not
# The dataframe 'df_all' consists of the predictors and target variable
df_all <- drug_use[, c(demographic_cols, youth_experience_cols, "ALCFLAG")]
df_all <- na.omit(df_all)

df_all$alcohol_use <- factor(df_all$ALCFLAG, levels = c(0, 1), labels = c("No", "Yes"))
df_all$ALCFLAG <- NULL  


# To readability, I'm renaming the predictors
colnames(df_all)[colnames(df_all) == "STNDALC"]     <- "Friend_Drinks_Daily"
colnames(df_all)[colnames(df_all) == "YFLMJMO"]     <- "Friend_Consumes_Marijuana_Monthly"
colnames(df_all)[colnames(df_all) == "YFLTMRJ2"]    <- "Friend_Offers_Marijuana"
colnames(df_all)[colnames(df_all) == "FRDMEVR2"]    <- "Friend_Ever_Smoked"
colnames(df_all)[colnames(df_all) == "STNDSMJ"]     <- "Friend_Smokes_Marijuana"
colnames(df_all)[colnames(df_all) == "EDUSCHGRD2"]  <- "Grade_Level"
colnames(df_all)[colnames(df_all) == "NEWRACE2"]    <- "Race"

# For plot readability, recoding categorical variables.
df_all$Friend_Drinks_Daily <- factor(df_all$Friend_Drinks_Daily, levels = c(1, 2), labels = c("Yes", "No"))

df_all$Friend_Consumes_Marijuana_Monthly <- factor(df_all$Friend_Consumes_Marijuana_Monthly, levels = c(1, 2), labels = c("Yes", "No"))

df_all$Friend_Offers_Marijuana <- factor(df_all$Friend_Offers_Marijuana, levels = c(1, 2), labels = c("Yes", "No"))

# Plotting the decision tree
tree_one <- tree(alcohol_use ~ ., data = df_all)
tree_one
summary(tree_one)
plot(tree_one)
text(tree_one, pretty = 0)
title("Decision Tree for binary classification: Predicting Youth Alcohol Consumption")

# Pruning the above tree
# Finding optimal size, using cross validation
set.seed(1)
cv_one <- cv.tree(tree_one, FUN = prune.misclass)
plot(cv_one$size, cv_one$dev, type = "b", main = "Cross-Validation", xlab = "Tree Size", ylab = "Misclassification Error")
opt_size <- cv_one$size[which.min(cv_one$dev)]
opt_size
# We got best size as 4
# But, 3 is the best optimal size from the graph
# So, we use 3 as the opt_size
prune_one <- prune.misclass(tree_one, best = 3)
plot(prune_one)
text(prune_one, pretty = 0)
title(paste("Pruned Decision Tree (Size =", 3, ")"))


# DECISION TREE ENSEMBLE METHODS

# Getting the numeric version of alcohol_use(categorical)
df_all$alcohol_use_num <- ifelse(df_all$alcohol_use == "Yes", 1, 0)

# Splitting the data into train and test data.
set.seed(42)
train_data <- sample(1:nrow(df_all), 0.6 * nrow(df_all))
train_set <- df_all[train_data, ]
test_set  <- df_all[-train_data, ]

# BAGGING
bag_one <- randomForest(alcohol_use ~ ., data = train_set[, -which(names(train_set) == "alcohol_use_num")], mtry = ncol(train_set) - 2, importance = TRUE, ntree = 500)
prediction_bag <- predict(bag_one, test_set)
cat("Bagging Accuracy:", mean(prediction_bag == test_set$alcohol_use), "\n")

# BOOSTING
set.seed(1)
boost_one <- gbm(alcohol_use_num ~ ., data = train_set[, -which(names(train_set) == "alcohol_use")], distribution = "bernoulli", n.trees = 1000, shrinkage = 0.01, interaction.depth = 3)
summary(boost_one)

probability_one <- predict(boost_one, test_set, n.trees = 1000, type = "response")
prediction_boost <- ifelse(probability_one > 0.5, "Yes", "No")
cat("Boosting Accuracy:", mean(prediction_boost == test_set$alcohol_use), "\n")

# RANDOM FOREST
rf_one <- randomForest(alcohol_use ~ ., data = train_set[, -which(names(train_set) == "alcohol_use_num")], mtry = 5, importance = TRUE, ntree = 500)
# mtry = 5 since we took sqrt(p)
prediction_rf <- predict(rf_one, test_set)
confusionMatrix(prediction_rf, test_set$alcohol_use)
cat("Random Forest Accuracy:", mean(prediction_rf == test_set$alcohol_use), "\n")
varImpPlot(rf_one, n.var = 5, sort = TRUE, main = "Top 5 Important Variables")


# plotting the three models accuracy results
models <- c("Boosting", "Random Forest", "Bagging")
accuracies <- c(80.7, 79.9, 79.1)/100
df_accuracies <- data.frame(models, accuracies)
ggplot(df_accuracies, aes(x = models, y = accuracies, fill = models)) +
  geom_col(width = 0.5, show.legend = TRUE) +
  labs(title = "Model Accuracies and their comparison for binary classification model",
       x = "Model", y = "Accuracy (%)") +
  theme_minimal()
```
Ensemble models were compared using accuracy:
Boosting: 80.7%
Random Forest: 79.9%
Bagging: 79.1%

And from the variable plot, Friend Drinks Daily, Friend Offers Marijuana, and Grade Level are the most important predictors identified by the model

```{r}
# PART 2
# MULTI-CLASS CLASSIFICATION: how often they used marijuana over the last year 

# Converting MRJYDAYS to numeric and also performing some data cleaning by including data, like these: 991, 993, 994, 997, 998 
drug_use$MRJYDAYS <- as.numeric(as.character(drug_use$MRJYDAYS))

# Imputation with mean for 900-level codes
invalid_codes <- c(991, 993, 994, 997, 998)
valid_mean <- mean(drug_use$MRJYDAYS[!drug_use$MRJYDAYS %in% invalid_codes], na.rm = TRUE)
drug_use$MRJYDAYS[drug_use$MRJYDAYS %in% invalid_codes] <- valid_mean

# Now, binning the data into 6 categories
drug_use$marijuana_use_level <- cut(
  drug_use$MRJYDAYS,
  breaks = c(-1, 0, 5, 15, 30, 90, 365),
  labels = c("NEVER", "RARE", "OCCASIONAL", "REGULAR", "FREQUENT", "DAILY"),
  right = TRUE
)

# The dataframe 'df_multi' consists of the predictors and target variable for multi-classification
df_multi <- drug_use[, c(demographic_cols, youth_experience_cols, "marijuana_use_level")]
df_multi <- na.omit(df_multi)

# Renaming variables for clarity
colnames(df_multi)[colnames(df_multi) == "FRDMJMON"] <- "Friend_Uses_Marijuana_Monthly"
colnames(df_multi)[colnames(df_multi) == "STNDSMJ"]  <- "Friend_Smokes_Marijuana"
colnames(df_multi)[colnames(df_multi) == "YFLMJMO"]    <- "Friend_Consumes_Marijuana_Monthly"
colnames(df_multi)[colnames(df_multi) == "FRDMEVR2"]   <- "Friend_Ever_Tried_Marijuana"
colnames(df_multi)[colnames(df_multi) == "YOSELL2"]    <- "Youth_Sold_Drugs"
colnames(df_multi)[colnames(df_multi) == "EDUSCHGRD2"] <- "Grade_Level"
colnames(df_multi)[colnames(df_multi) == "YFLTMRJ2"]   <- "Friend_Offered_Marijuana"
colnames(df_multi)[colnames(df_multi) == "NEWRACE2"]   <- "Race"

# For plot readability, recoding categorical variables.
df_multi$Friend_Uses_Marijuana_Monthly <- factor(df_multi$Friend_Uses_Marijuana_Monthly, levels = c(1, 2), labels = c("Yes", "No"))

df_multi$Friend_Smokes_Marijuana <- factor(df_multi$Friend_Smokes_Marijuana, levels = c(1, 2), labels = c("Yes", "No"))

df_multi$Friend_Consumes_Marijuana_Monthly <- factor(df_multi$Friend_Consumes_Marijuana_Monthly, levels = c(1, 2), labels = c("Yes", "No"))

df_multi$Friend_Ever_Tried_Marijuana <- factor(df_multi$Friend_Ever_Tried_Marijuana, levels = c(1, 2), labels = c("Yes", "No"))

df_multi$Youth_Sold_Drugs <- factor(df_multi$Youth_Sold_Drugs, levels = c(1, 2), labels = c("Yes", "No"))

df_multi$Grade_Level <- factor(df_multi$Grade_Level, levels = c(1:8, 9:11, 98, 99),
                               labels = c(
                                 rep("School", 8),  
                                 rep("College", 3),        
                                 "No Answer", "Skipped"
                               )
)

df_multi$Friend_Offered_Marijuana <- factor(df_multi$Friend_Offered_Marijuana, levels = c(1, 2), labels = c("Yes", "No"))

# Plotting the decision tree
tree_two <- tree(marijuana_use_level ~ ., data = df_multi)
tree_two
summary(tree_two)
plot(tree_two)
text(tree_two, pretty = 0)
title("Decision Tree for model two is Marijuana Used by Youth into 6 Categories")

# Pruning the above tree
# Finding optimal tree size, we use cross validation
set.seed(1)
cv_two <- cv.tree(tree_two, FUN = prune.misclass)
plot(cv_two$size, cv_two$dev, type = "b", main = "CV: Marijuana Use Tree", xlab = "Tree Size", ylab = "Misclassification Error")
opt_size <- cv_two$size[which.min(cv_two$dev)]
opt_size

# 3 is the best optimal size from the graph
# So, we use 3 as the opt_size
prune_two <- prune.misclass(tree_two, best = 3)
plot(prune_two)
text(prune_two, pretty = 0)
title(paste("The Pruned Tree of Multi-classification model is of (Size =", 3, ")"))

# DECISION TREE ENSEMBLE METHODS
# Splitting the data into train and test data.
set.seed(123)
train_data <- createDataPartition(df_multi$marijuana_use_level, p = 0.8, list = FALSE)
train_set <- df_multi[train_data, ]
test_set  <- df_multi[-train_data, ]

# Plotting the tree
tree_two <- predict(prune_two, test_set, type = "class")
mean(tree_two == test_set$marijuana_use_level)

# RANDOM FOREST
# Dropping unused factor levels
train_set$marijuana_use_level <- droplevels(train_set$marijuana_use_level)
test_set$marijuana_use_level <- droplevels(test_set$marijuana_use_level)

set.seed(42)
# mtry is set to 5, as we took sqrt(p)
rf_two <- randomForest(marijuana_use_level ~ ., data = train_set, mtry = 5, importance = TRUE, ntree = 500)

prediction_two <- predict(rf_two, test_set, type = "class")
prediction_two <- factor(prediction_two, levels = levels(test_set$marijuana_use_level))
cat("Random Forest Accuracy:", mean(prediction_two == test_set$marijuana_use_level), "\n")
confusionMatrix(prediction_two, test_set$marijuana_use_level)
varImpPlot(rf_two, n.var = 5, main = "Top 5 Important Variables - Random Forest")
```
In the multi-class classification, the initial decision tree misclassified 12.5 percent of cases, and pruning increased accuracy to 86.6 percent  

Random Forest achieved 89.3 percent accuracy with a balanced accuracy of 66.3 percent  

Friends consuming marijuana emerged as the most powerful predictor of usage frequency.


```{r}
# PART3: REGRESSION: number of days per year a person has consumed alcohol 
# Performing some data cleaning on IRALCFM, and ignoring 91 values, which is "did not drink", so we set them to 0.
drug_use$alcohol_days_past_month <- ifelse(
  drug_use$IRALCFM %in% 1:30,
  drug_use$IRALCFM,
  ifelse(drug_use$IRALCFM %in% 91,0,NA)
)

# The dataframe 'df_reg' consists of the predictors and outcome variables for the regression model and then remove the null values
df_reg <- drug_use[, c(demographic_cols, youth_experience_cols, "alcohol_days_past_month")]
df_reg <- na.omit(df_reg)
print(table(df_reg$alcohol_days_past_month))

# For readability and clarity of predictor variable names
# Rename variables in df_reg for better readability
colnames(df_reg)[colnames(df_reg) == "FRDMJMON"] <- "Friend_Uses_Marijuana_Monthly"
colnames(df_reg)[colnames(df_reg) == "STNDSMJ"]  <- "Friend_Smokes_Marijuana"
colnames(df_reg)[colnames(df_reg) == "RLGFRND"]  <- "Religious_Friend"
colnames(df_reg)[colnames(df_reg) == "YFLTMRJ2"] <- "Friend_Influence_Marijuana"
colnames(df_reg)[colnames(df_reg) == "FRDMEVR2"] <- "Friend_Ever_Used_Marijuana"
colnames(df_reg)[colnames(df_reg) == "HEALTH2"]  <- "Self_Reported_Health"
colnames(df_reg)[colnames(df_reg) == "YOSELL2"]  <- "Youth_Sold_Drugs"
colnames(df_reg)[colnames(df_reg) == "STNDALC"]  <- "Friend_Drinks_Daily"
colnames(df_reg)[colnames(df_reg) == "EDUSCHGRD2"] <- "Grade_Level"
colnames(df_reg)[colnames(df_reg) == "NEWRACE2"] <- "Race_Category"
colnames(df_reg)[colnames(df_reg) == "ARGUPAR"]  <- "Argued_With_Parents"


# For plot readability, recoding categorical variables.
df_reg$Friend_Drinks_Daily <- factor(df_reg$Friend_Drinks_Daily, levels = c(1, 2), labels = c("Yes", "No"))

df_reg$Youth_Sold_Drugs <- factor(df_reg$Youth_Sold_Drugs, levels = c(1, 2), labels = c("Yes", "No"))

df_reg$Argued_With_Parents <- factor(df_reg$Argued_With_Parents, levels = c(1, 2), labels = c("Yes", "No"))


# Splitting the data into train and test data, 70% train data and 30% test data
set.seed(42)
train_index <- sample(1:nrow(df_reg), 0.7 * nrow(df_reg))
train_set <- na.omit(df_reg[train_index, ])
test_set  <- na.omit(df_reg[-train_index, ])

# DECISION TREE
tree_three <- tree(alcohol_days_past_month ~ ., data = train_set)
tree_three
summary(tree_three)
plot(tree_three)
text(tree_three, pretty = 0)
title("Regression Tree: Past-Month Alcohol Consumption by Youth")

prediction_three <- predict(tree_three, test_set)
mean((prediction_three - test_set$alcohol_days_past_month)^2)

# PRUNING THE TREE
set.seed(123)
cv_three <- cv.tree(tree_three)
plot(cv_three$size, cv_three$dev, type = "b", xlab = "Tree Size", ylab = "Deviance", main = "CV for Regression Tree")

opt_size <- cv_three$size[which.min(cv_three$dev)]
cat("Optimal Tree Size:", opt_size, "\n")
# so we take 4 from the graph
prune_three <- prune.tree(tree_three, best = 4)
summary(prune_three)
plot(prune_three)
text(prune_three, pretty = 0)
title("Pruned Regression Tree")

prediction_three <- predict(prune_three, test_set)
mean((prediction_three - test_set$alcohol_days_past_month)^2)
mse <- mean((prediction_three - test_set$alcohol_days_past_month)^2)
rss <- sum((prediction_three - test_set$alcohol_days_past_month)^2)
tss <- sum((test_set$alcohol_days_past_month - mean(test_set$alcohol_days_past_month))^2)
r_squared <- 1 - rss/tss

cat("MSE is:", mse, "\n")
cat("R squared is:", r_squared, "\n")
# Note: The outcome is in days
```

```{r}
# BAGGING
# Using all predictors for bagging 
bag_three <- randomForest(alcohol_days_past_month ~ ., data = train_set, mtry = ncol(train_set) - 1, importance = TRUE, ntree = 500)
```

```{r}
prediction_bg <- predict(bag_three, test_set, type = "class")
```

```{r}
mean((prediction_bg - test_set$alcohol_days_past_month)^2)
```

```{r}
varImpPlot(bag_three, n.var = 5, sort = TRUE, main = "Top 5 Important Variables (Regression)")
```

